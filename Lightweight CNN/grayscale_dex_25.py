import tensorflow as tf
import PIL
from PIL import Image
import csv
from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import AveragePooling2D
from keras.layers import DepthwiseConv2D
from keras.layers import Concatenate
from keras.layers import BatchNormalization
from keras.layers import ReLU
from keras.layers import Layer
from keras.layers import Dropout
from keras.layers import GlobalMaxPooling2D
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.metrics import Precision, Recall
import pathlib
import numpy as np

def channel_shuffle(x, groups):
    batch_size, height, width, num_channels = tf.shape(x)
    channels_per_group = num_channels // groups
    x = tf.reshape(x, (batch_size, height, width, groups, channels_per_group))
    x = tf.transpose(x, (0, 1, 2, 4, 3))
    x = tf.reshape(x, (batch_size, height, width, num_channels))
    return x

def channel_split(x, groups):
    batch_size, height, width, num_channels = tf.shape(x)
    channels_per_group = num_channels // groups
    split_tensors = tf.split(x, groups, axis=-1)
    return split_tensors




data_dir = pathlib.Path(r"/home/bmitra/Soumil/attempt/drebin_top20_images_resized/grayscale_dex/train")        # turning into a Python path
class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))


train_dir = r"/home/bmitra/Soumil/attempt/drebin_top20_images_resized/grayscale_dex/train"
validation_dir = r"/home/bmitra/Soumil/attempt/drebin_top20_images_resized/grayscale_dex/val"


train_datagen = ImageDataGenerator(rescale=1/255.)
validation_datagen = ImageDataGenerator(rescale=1/255.)

# Load data from directories and turn it into batches
train_data = train_datagen.flow_from_directory(train_dir,
                                               target_size=(224, 224),
                                               batch_size=4,
                                               color_mode='grayscale',
                                               class_mode='categorical')

validation_data = validation_datagen.flow_from_directory(validation_dir,
                                              target_size=(224, 224),
                                               color_mode='grayscale',
                                              class_mode='categorical')

input = Input(shape=(224,224,1,))
conv1 = Conv2D(24, kernel_size=3, strides=2)(input)
pool1 = MaxPooling2D(pool_size=3, strides=2)(conv1)

dwconvl = DepthwiseConv2D(kernel_size=3, strides=2)(pool1)
bl = BatchNormalization()(dwconvl)
convl = Conv2D(116, kernel_size=1)(bl)
bl = BatchNormalization()(convl)
rl = ReLU()(bl)

convr = Conv2D(116, kernel_size=1)(pool1)
br = BatchNormalization()(convr)
rr = ReLU()(br)
dwconvr = DepthwiseConv2D(kernel_size=3, strides=2)(rr)
br = BatchNormalization()(dwconvr)
convr = Conv2D(116, kernel_size=1)(br)
br = BatchNormalization()(convr)
rr = ReLU()(br)

concat = Concatenate()([rl, rr])

shuffled = channel_shuffle(concat, 116)

l, r = channel_split(shuffled, 2)

#pooll = AveragePooling2D(pool_size=3, strides=2)(l)

convr = Conv2D(116, kernel_size=1, padding='same')(r)
br = BatchNormalization()(convr)
rr = ReLU()(br)
dwconvr = DepthwiseConv2D(kernel_size=3, strides=1, padding='same')(rr)
br = BatchNormalization()(dwconvr)
convr = Conv2D(116, kernel_size=1, padding='same')(br)
br = BatchNormalization()(convr)
rr = ReLU()(br)

concat = Concatenate()([l, rr])
shuffled = channel_shuffle(concat, 116)
conv2 = Conv2D(1024, kernel_size=1)(shuffled)
pool2 = GlobalMaxPooling2D()(conv2)
dropout = Dropout(0.25)(pool2)
flat = Flatten()(dropout)
output = Dense(20, activation='softmax')(flat)
model = Model(inputs=input, outputs=output)


# Compile the model
model.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=["accuracy", Precision(name='precision'), Recall(name='recall')])

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    't20grayscale_dex_25.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1
)

# Fit the model
history = model.fit(train_data,
                    epochs=30,
                    validation_data=validation_data,  # Use the validation data
                    callbacks=[checkpoint_callback])  # Add the checkpoint callback

print("Validation accuracy")
print(history.history['val_accuracy'])
print("Validation Loss")
print(history.history['val_loss'])
print("Validation Precision")
print(history.history['val_precision'])
print("Validation Recall")
print(history.history['val_recall'])

with open('t20grayscale_dex_25.csv', mode='w') as file:
    writer = csv.writer(file)
    writer.writerow(['Validation accuracy', 'Validation Loss', 'Validation Precision', 'Validation Recall'])
    for i in range(len(history.history['val_accuracy'])):
        writer.writerow([history.history['val_accuracy'][i], history.history['val_loss'][i], history.history['val_precision'][i], history.history['val_recall'][i]])

